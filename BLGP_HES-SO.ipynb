{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood glucose level challenge - A Deep Learning Approach for Blood Glucose Prediction of Type 1 Diabetes\n",
    "\n",
    "## Authors\n",
    "\n",
    "* Albertetti Fabrizio\n",
    "* Freiburghaus Jonas\n",
    "* Rizzotti AÃ¯cha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Conv1D, Dense, Input, MaxPooling1D, LSTM\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset\n",
    "\n",
    "Read the four used features from an XML file :\n",
    "\n",
    "* Glucose level\n",
    "* Basal\n",
    "* Bolus\n",
    "* Meal\n",
    "\n",
    "It is possible to add other available features by reading them in the same manner.  \n",
    "All the resulting dataframes are then merged in one single dataframe.\n",
    "\n",
    "The folder structure should be as following :\n",
    "\n",
    "* Blgp.ipynb\n",
    "* OhioT1DM-testing\n",
    "    * 559-ws-testing.xml\n",
    "    * 563-ws-testing.xml\n",
    "    * 570-ws-testing.xml\n",
    "    * 575-ws-testing.xml\n",
    "    * 588-ws-testing.xml\n",
    "    * 591-ws-testing.xml\n",
    "* OhioT1DM-training\n",
    "    * 559-ws-training.xml\n",
    "    * 563-ws-training.xml\n",
    "    * 570-ws-training.xml\n",
    "    * 575-ws-training.xml\n",
    "    * 588-ws-training.xml\n",
    "    * 591-ws-training.xml\n",
    "* OhioT1DM-2-testing\n",
    "    * 540-ws-testing.xml\n",
    "    * 544-ws-testing.xml\n",
    "    * 552-ws-testing.xml\n",
    "    * 567-ws-testing.xml\n",
    "    * 584-ws-testing.xml\n",
    "    * 596-ws-testing.xml\n",
    "* OhioT1DM-2-training\n",
    "    * 540-ws-training.xml\n",
    "    * 544-ws-training.xml\n",
    "    * 552-ws-training.xml\n",
    "    * 567-ws-training.xml\n",
    "    * 584-ws-training.xml\n",
    "    * 596-ws-training.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cgm(soup):\n",
    "    df_cgm = pd.DataFrame([event.attrs for event in soup.find('glucose_level').find_all('event')])\n",
    "    df_cgm['ts'] = pd.to_datetime(df_cgm['ts'], format='%d-%m-%Y %H:%M:%S')\n",
    "    df_cgm.index = df_cgm['ts']\n",
    "    df_cgm.drop('ts', axis=1, inplace=True)\n",
    "    df_cgm.rename(columns={'value': 'glucose_level'}, inplace=True)\n",
    "    df_cgm['glucose_level'] = df_cgm['glucose_level'].apply(np.float32)\n",
    "    return df_cgm\n",
    "\n",
    "def read_basal(soup):\n",
    "    df_basal = pd.DataFrame([event.attrs for event in soup.find('basal').find_all('event')])\n",
    "    df_basal['ts'] = pd.to_datetime(df_basal['ts'], format='%d-%m-%Y %H:%M:%S')\n",
    "    df_basal.index = df_basal['ts']\n",
    "    df_basal.drop('ts', axis=1, inplace=True)\n",
    "    df_basal.rename(columns={'value': 'basal'}, inplace=True)\n",
    "    df_basal['basal'] = df_basal['basal'].apply(np.float32)\n",
    "    return df_basal\n",
    "\n",
    "def read_bolus(soup):\n",
    "    df_bolus = pd.DataFrame([event.attrs for event in soup.find('bolus').find_all('event')])\n",
    "    df_bolus.rename(columns={'ts_begin': 'ts', 'dose': 'bolus'}, inplace=True)\n",
    "    df_bolus['ts'] = pd.to_datetime(df_bolus['ts'], format='%d-%m-%Y %H:%M:%S')\n",
    "    df_bolus.index = df_bolus['ts']\n",
    "    df_bolus.drop(['ts', 'ts_end', 'type'], axis=1, inplace=True)\n",
    "    df_bolus['bolus'] = df_bolus['bolus'].apply(np.float32)\n",
    "    return df_bolus\n",
    "\n",
    "def read_meal(soup):\n",
    "    try:\n",
    "        df_meal = pd.DataFrame([event.attrs for event in soup.find('meal').find_all('event')])\n",
    "        df_meal['ts'] = pd.to_datetime(df_meal['ts'], format='%d-%m-%Y %H:%M:%S')\n",
    "        df_meal.index = df_meal['ts']\n",
    "        df_meal.drop(['ts', 'type'], axis=1, inplace=True)\n",
    "        df_meal['carbs'] = df_meal['carbs'].apply(np.float32)\n",
    "        return df_meal\n",
    "    except KeyError:\n",
    "        df_meal = pd.DataFrame({'ts': [], 'carbs': []})\n",
    "        df_meal.index = df_meal['ts']\n",
    "        return df_meal.drop('ts', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_contributor(contrib_dir, contrib_file):\n",
    "    with open(os.path.join(contrib_dir, contrib_file)) as file:\n",
    "        soup = BeautifulSoup(file.read(), 'lxml')\n",
    "\n",
    "        df_cgm = read_cgm(soup)\n",
    "        df_basal = read_basal(soup)\n",
    "        df_bolus = read_bolus(soup)\n",
    "        df_meal = read_meal(soup)\n",
    "\n",
    "        df_contributor = df_meal.join([df_bolus, df_basal, df_cgm], how='outer')\n",
    "        \n",
    "        resample_index = pd.date_range(start= df_contributor.index[0], end= df_contributor.index[-1], freq='5T')\n",
    "        df_dummy = pd.DataFrame(np.NaN, index=resample_index, columns= df_contributor.columns)\n",
    "        df_contributor =  df_contributor.combine_first(df_dummy)\n",
    "        \n",
    "        return df_contributor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_directory(contrib_dir):\n",
    "    contrib_files = {contrib_file.split('-')[0]:contrib_file for contrib_file in os.listdir(contrib_dir)}\n",
    "    \n",
    "    contrib_dataframes = {}\n",
    "    for contrib_id, contrib_file in contrib_files.items():\n",
    "        contrib_dataframes[contrib_id] = parse_contributor(contrib_dir, contrib_file)\n",
    "        \n",
    "    return contrib_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_train_stage1 = parse_directory('OhioT1DM-training')\n",
    "all_df_test_stage1 = parse_directory('OhioT1DM-testing')\n",
    "all_df_train_stage2 = parse_directory('OhioT1DM-2-training')\n",
    "all_df_test_stage2 = parse_directory('OhioT1DM-2-testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The preprocessing is summarized in the following steps :\n",
    "\n",
    "For the features :\n",
    "\n",
    "1. Save the indexes where the glucose level is available\n",
    "2. Resample the dataframe to an 1 second delta time\n",
    "3. Forward fill the values\n",
    "4. Fill the left NaN values with 0\n",
    "5. Resample to a 5 minutes delta time\n",
    "6. In order to preserve as much information as possible. The carbs, basal and bolus values are assigned to the closest past time point.\n",
    "7. A custom keras TimeseriesGenerator is implemented to smooth the window of the past 2 hours worth data. We use a Gaussian smoothing with std = 1\n",
    "\n",
    "For the targets :\n",
    "\n",
    "Compute the difference from the last blood glucose level known.\n",
    "\n",
    "$$y_{t+L} = bg_{t+L} - bg_{t}, \\mathrm{for} \\ L = 1, 2, \\ \\dots, 12$$\n",
    "\n",
    "Where $bg_t$ is the blood glucose value at time $t$, $L$ is the lag value in timesteps for the horizon, and $y$ the label to predict, that is the differentiated value of the blood glucose level.\n",
    "\n",
    "The model will predict the blood glucose level difference and the predicted blood glucose is then obtained by :\n",
    "\n",
    "$$\\widehat{bg}_{t+L} = bg_{t} + \\hat{y}_{t+L}, \\mathrm{for} \\ L = 1, 2, \\ \\dots, 12$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_series(df):\n",
    "    interest_index = df[~df['glucose_level'].isna()].index\n",
    "    df_cp = df.copy(deep=True)\n",
    "    \n",
    "    df = df.resample('1S').pad()\n",
    "    df = df.fillna(method='ffill').fillna(0)\n",
    "    df.loc[df.index[-1] + pd.Timedelta('5T'), 'glucose_level'] = df.iloc[-1]['glucose_level']\n",
    "    df = df.resample('5T').first().shift(-1)\n",
    "    \n",
    "    for feature in ['carbs', 'basal', 'bolus']:\n",
    "        locs = [df.index.get_loc(idx, method='ffill') for idx in df_cp[~df_cp[feature].isna()].index[1:]]\n",
    "        mask_df = df.reset_index().index.isin(locs)\n",
    "        df.loc[~mask_df, feature] = 0\n",
    "        \n",
    "    return df, interest_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_target(df, lag=12, target_col='glucose_level'):\n",
    "    target_df = pd.concat([-df[target_col].diff(periods=-i) for i in range(1, lag + 1)], axis=1).dropna(axis=0)\n",
    "    df = df.iloc[:len(target_df)]\n",
    "\n",
    "    return df, target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedTimeseriesGenerator(TimeseriesGenerator):\n",
    "    \n",
    "    def __init__(self, sigma=1, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = super().__getitem__(idx)\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[2]):\n",
    "                x[i, :, j] = gaussian_filter1d(x[i, :, j], self.sigma)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "def make_generator(X, Y, horizon_window=24, sampling_rate=1, batch_size=1024):\n",
    "    generator = SmoothedTimeseriesGenerator(data=X, targets=Y.shift(1).values, length=horizon_window, sampling_rate=sampling_rate, batch_size=batch_size)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "In the paper the reported persistent algorithm does not use the time aligned features.\n",
    "\n",
    "Persistent algorithm :\n",
    "\n",
    "$$\\hat{y}_{t+L} = bg_{t-1}, \\mathrm{for} \\ L = 1, 2, \\ \\dots, 12$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_persistence_metrics(all_df_test):\n",
    "    pred_times = list(range(1, 13))\n",
    "    mae_dict = {f'mae+{i}' : [] for i in pred_times}\n",
    "    rmse_dict = {f'rmse+{i}' : [] for i in pred_times}\n",
    "    \n",
    "    for df_test in tqdm(all_df_test.values()):\n",
    "        df_test, test_interest_index = align_series(df_test)\n",
    "        \n",
    "        for i in pred_times:\n",
    "            df_test_cp = df_test.copy(deep=True)\n",
    "            df_test_cp[f'glucose_level+{i}'] = df_test_cp['glucose_level'].shift(i)\n",
    "            df_test_cp = df_test_cp.resample('1S').ffill() \n",
    "            \n",
    "            df_pred = df_test_cp.loc[test_interest_index].copy(deep=True)\n",
    "            df_pred.dropna(inplace=True)\n",
    "            y_true = df_pred['glucose_level'].values\n",
    "            y_pred = df_pred[f'glucose_level+{i}'].values\n",
    "            \n",
    "            rmse = np.sqrt(\n",
    "                mean_squared_error(\n",
    "                    y_true,\n",
    "                    y_pred\n",
    "                )\n",
    "            )\n",
    "            rmse_dict[f'rmse+{i}'].append(rmse)\n",
    "\n",
    "            mae = mean_absolute_error(\n",
    "                y_true, \n",
    "                y_pred\n",
    "            )\n",
    "            mae_dict[f'mae+{i}'].append(mae)\n",
    "            \n",
    "    df_rmse = pd.DataFrame(rmse_dict)\n",
    "    df_mae = pd.DataFrame(mae_dict)\n",
    "    \n",
    "    return df_rmse, df_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse, df_mae = compute_persistence_metrics(all_df_test_stage2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mae.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaLSTM(LSTM):\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(DeltaLSTM, self).__init__(units, **kwargs)\n",
    "\n",
    "    def step(self, x, states):\n",
    "        h, [h, c] = super(DeltaLSTM, self).step(x, states)\n",
    "        o = h / self.activation(c)\n",
    "        h = K.multiply(o, K.tanh(c))\n",
    "        y = K.update_add(K.multiply([o, K.tanh(c)]), x)\n",
    "\n",
    "        return y, [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_timesteps=24, n_features=4):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(filters=8, kernel_size=24, padding='same', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(filters=16, kernel_size=12, padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(filters=32, kernel_size=6, padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(DeltaLSTM(64))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(12))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='RMSProp',\n",
    "        loss='mean_absolute_error',\n",
    "        metrics=['mean_absolute_error'],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Ohio T1DM dataset\n",
    "\n",
    "If desired a model may be pretrained using the first Ohio T1DM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_model(all_train_df, all_test_df, epochs=1000, shuffle=False):\n",
    "    COLUMNS = ['carbs', 'bolus', 'basal', 'glucose_level']\n",
    "    cb_lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor= 0.1, patience=3, min_lr= 1e-9)\n",
    "    cb_early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=50,\n",
    "        verbose=0,\n",
    "        mode='auto',\n",
    "        baseline=None,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    crnn_model = build_model()\n",
    "    \n",
    "    for train_contrib_id, test_contrib_id in zip(all_train_df.keys(), all_test_df.keys()):\n",
    "        df_train = all_train_df[train_contrib_id]\n",
    "        df_train['glucose_level'] = df_train['glucose_level'].interpolate('linear')\n",
    "        df_train, train_interest_index = align_series(df_train)\n",
    "        df_train.dropna(inplace=True)\n",
    "        X_train, Y_train = lag_target(df_train[COLUMNS])\n",
    "        train_generator = make_generator(X_train.values, Y_train)\n",
    "\n",
    "        df_test = all_test_df[test_contrib_id]\n",
    "        df_test, test_interest_index = align_series(df_test)\n",
    "        df_test.dropna(inplace=True)\n",
    "        X_test, Y_test = lag_target(df_test[COLUMNS])\n",
    "        test_generator = make_generator(X_test.values, Y_test)\n",
    "        \n",
    "        print(f'Fitting contributor : {train_contrib_id}')\n",
    "\n",
    "        crnn_model.fit_generator(train_generator,\n",
    "                                 steps_per_epoch=len(train_generator),\n",
    "                                 epochs=epochs,\n",
    "                                 validation_data=test_generator,\n",
    "                                 shuffle=shuffle,\n",
    "                                 callbacks=[cb_lr_reducer, cb_early_stopping],\n",
    "                                 verbose=1)\n",
    "        \n",
    "    return crnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn_model = pretrain_model(all_df_train_stage1, all_df_test_stage1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn_model.save('rcnn_pretrain.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Ohio T1DM dataset\n",
    "\n",
    "The pretrained model is loaded for each contributor and trained over 1000 epochs with a batch size of 1024.  \n",
    "Early stopping is used to regularize the model. The patience is set to 50 epcohs and the weights from the model with the best validation loss are restored.  \n",
    "Before early stopping a learning rate reducer is used with a patience of 15 epochs.  \n",
    "The train-validation split is performed chronologically wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(crnn_model, times, test_generator, df_test, horizon_window=24):\n",
    "    y_pred = crnn_model.predict_generator(test_generator)\n",
    "    df_pred = df_test.copy()\n",
    "    for time_idx, time in times.items():\n",
    "        pred_col = f\"pred_cgm_{time}\"\n",
    "        df_pred['shift'] = df_pred['glucose_level'].shift(time + 1)\n",
    "        df_pred[pred_col] = df_pred['shift'][horizon_window:] + y_pred[:, time]\n",
    "        df_pred = df_pred.drop(columns=['shift'])\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(times, df_pred, horizon_window=24):\n",
    "    rmse_by_time = []\n",
    "    mae_by_time = []\n",
    "    for time_idx, time in times.items():\n",
    "        pred_col = f\"pred_cgm_{time}\"\n",
    "        start_idx = horizon_window + time\n",
    "\n",
    "        mse = mean_squared_error(df_pred[pred_col], df_pred['glucose_level'])\n",
    "        rmse_by_time.append(np.sqrt(mse))\n",
    "\n",
    "        mae = mean_absolute_error(df_pred[pred_col], df_pred['glucose_level'])\n",
    "        mae_by_time.append(mae)\n",
    "\n",
    "    return pd.DataFrame({\"Times\": list(times.keys()), \"RMSE\": rmse_by_time, \"MAE\": mae_by_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_df(df, path):\n",
    "    df.to_csv(path, sep=',', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    contributor, \n",
    "    index_of_interest, \n",
    "    crnn_model, \n",
    "    save_dir, \n",
    "    test_generator, \n",
    "    df_test,  \n",
    "    test_offset=12\n",
    "):\n",
    "    print(f'Saving metric for contributor : {contributor}')\n",
    "    times = {\n",
    "        '5': 0,\n",
    "        '10': 1,\n",
    "        '15': 2,\n",
    "        '20': 3,\n",
    "        '25': 4,\n",
    "        '30': 5,\n",
    "        '35': 6,\n",
    "        '40': 7,\n",
    "        '45': 8,\n",
    "        '50': 9,\n",
    "        '55': 10,\n",
    "        '60': 11\n",
    "    }\n",
    "\n",
    "    pred_df = run_predictions(crnn_model, times, test_generator, df_test)\n",
    "    pred_df = pred_df.resample('1S').ffill()\n",
    "    pred_df = pred_df.loc[index_of_interest][test_offset:].dropna()\n",
    "    df_metric = compute_metric(times, pred_df)\n",
    "    \n",
    "    dump_df(df_metric, os.path.join(save_dir, f'{contributor}_metric.csv'))\n",
    "\n",
    "    for time, time_idx in times.items():\n",
    "        drop_time_columns = list(filter(lambda t: t != time_idx, times.values()))\n",
    "        drop_columns = ['bolus', 'basal', 'carbs']\n",
    "        drop_columns += list(map(lambda t: f'pred_cgm_{t}', drop_time_columns))\n",
    "        pred_t_df = pred_df.drop(columns=drop_columns)\n",
    "        pred_t_df.rename(columns={'cgm': 'true_BGL', f'pred_cgm_{time_idx}': 'predicted_BGL'}, inplace=True)\n",
    "        dump_df(pred_t_df, os.path.join(save_dir, f'1_{contributor}_{time}.csv'))\n",
    "        \n",
    "    return df_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_train_model(all_train_df, all_test_df, epochs=1000, shuffle=False):\n",
    "    COLUMNS = ['carbs', 'bolus', 'basal', 'glucose_level']\n",
    "    cb_lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor= 0.1, patience=15, min_lr= 1e-9)\n",
    "    cb_early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=50,\n",
    "        verbose=0,\n",
    "        mode='auto',\n",
    "        baseline=None,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    current_time = now.strftime('%Y_%m_%d_%H_%M')\n",
    "    save_dir = os.path.join('output', current_time)\n",
    "    os.makedirs(save_dir)\n",
    "    metrics = []\n",
    "    \n",
    "    for train_contrib_id, test_contrib_id in zip(all_train_df.keys(), all_test_df.keys()):\n",
    "        crnn_model = load_model('rcnn_pretrain.h5', custom_objects={'DeltaLSTM': DeltaLSTM})\n",
    "\n",
    "        df_train = all_train_df[train_contrib_id]\n",
    "        df_train['glucose_level'] = df_train['glucose_level'].interpolate('linear')\n",
    "        df_train, train_interest_index = align_series(df_train)\n",
    "        split_idx = int(len(df_train) * 0.8)\n",
    "        df_val = df_train.iloc[split_idx+48:]\n",
    "        df_train = df_train.iloc[:split_idx]\n",
    "        X_train, Y_train = lag_target(df_train[COLUMNS])\n",
    "        train_generator = make_generator(X_train.values, Y_train)\n",
    "        \n",
    "        \n",
    "        df_test = all_test_df[test_contrib_id]\n",
    "        df_test, test_interest_index = align_series(df_test)\n",
    "        df_test = pd.concat([df_train.dropna().iloc[-12:], df_test], axis=0)\n",
    "        \n",
    "        X_val, Y_val = lag_target(df_val[COLUMNS])\n",
    "        test_generator = make_generator(X_val.values, Y_val)\n",
    "        \n",
    "        print(f'Fitting contributor : {train_contrib_id}')\n",
    "        \n",
    "        crnn_model.fit_generator(train_generator,\n",
    "                                 steps_per_epoch=len(train_generator),\n",
    "                                 epochs=epochs,\n",
    "                                 validation_data=test_generator,\n",
    "                                 shuffle=shuffle,\n",
    "                                 callbacks=[cb_lr_reducer, cb_early_stopping],\n",
    "                                 verbose=1)\n",
    "        \n",
    "        evaluation_generator = SmoothedTimeseriesGenerator(\n",
    "            data=df_test[COLUMNS].values, \n",
    "            targets=np.zeros((len(df_test), 12), dtype=np.float32), \n",
    "            length=24, \n",
    "            sampling_rate=1, \n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        df_metric = evaluate(test_contrib_id, test_interest_index, crnn_model, save_dir, evaluation_generator, df_test[COLUMNS])\n",
    "        metrics.append(df_metric)\n",
    "        \n",
    "    print('Computing mean metric')\n",
    "    df_metrics = pd.concat(metrics)\n",
    "    df_metrics = df_metrics.groupby('Times').mean()\n",
    "    dump_df(df_metrics, os.path.join(save_dir, 'all_contrib_mean_metric.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_train_model(all_df_train_stage2, all_df_test_stage2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
